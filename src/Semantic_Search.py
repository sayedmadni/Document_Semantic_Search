#Trying to use GPU
import torch
import sys
from pathlib import Path
import streamlit as st
from check_toxicity import query_guadrails
from check_basic_rules import query_guadrails
from prompt_paraphase import paraphrase_sentence


DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using device: {DEVICE}")

# Add the src directory to the Python path for imports
sys.path.insert(0, str(Path(__file__).parent))

def run_search_app():
    """Launch the Streamlit search interface."""
    st.markdown("""
    <div style="background: linear-gradient(135deg, #f0f8ff 0%, #e6f3ff 100%); 
                border: 3px solid #7FB3D3; border-radius: 25px; 
                padding: 1.5rem; margin-bottom: 1rem; 
                box-shadow: 0 20px 40px rgba(127, 179, 211, 0.1);">
        <div style="text-align: center;">
            <h1 style="margin: 0 0 0.5rem 0; background: linear-gradient(90deg, #7FB3D3, #98D8C8, #7FB3D3); 
                       background-size: 200% 200%; -webkit-background-clip: text; 
                       -webkit-text-fill-color: transparent; background-clip: text; 
                       font-size: 3rem; font-weight: 700; animation: titleShimmer 2s ease-in-out infinite;">
                üîç College Admissions Search
            </h1>
            <h3 style="margin: 0; color: #2C3E50; font-style: italic; font-size: 1.2rem;">
                *Discover college admission guidance through AI-powered search*
            </h3>
        </div>
    </div>
    """, unsafe_allow_html=True)
        
    # Search section with enhanced styling
    st.markdown("## üîç AI-Powered Search")
    st.markdown("**Describe what you're looking for and let AI find the perfect answer to your question!**")
    
    # Initialize session state for search query
    if 'search_query' not in st.session_state:
        st.session_state.search_query = ""
    
    # User input with better layout - spread horizontally
    col1,col2 = st.columns([8,2])
    
    with col1:
        q = st.text_area(
            "üéØ Search Query", 
            value=st.session_state.search_query,
            placeholder="e.g., 'How can I prepare for the SAT?'",
            help="Describe the college admission info you're looking for in natural language",
            key="search_input",
            height=100
        ) 

    # Perform search when user enters a query
    if q:
        # Input validation and safety check       
        with st.spinner("Searching..."):
            try:
                print(f'Query: {q}')
                
                # Use Qdrant search function
                from search import search_qdrant
                search_results = search_qdrant(q)
                
                # Display search results
                #getting paraphrase questions for the input query
                paraphrased_questions = paraphrase_sentence(q)
                print("\nParaphrased Questions :: ")
                
                for i, final_output in enumerate(paraphrased_questions):
                    print(f"{i+1}: {final_output}")

                #need to check if we can pass the paraphrased_questions to the embedder.encode call

                #from sentence_transformers import util
                #search_results = util.semantic_search(query, embeddings, top_k = 3)
                # Display results with enhanced styling
                st.markdown("## üìã Search Results")
                
                # Collect all relevant chunks for LLM processing
                relevant_chunks = []
                for index, result in enumerate(search_results):
                    relevant_chunks.append(result)
                    
                    # Display individual results
                    with st.expander(f"üîç Result {index+1}", expanded=index==0):
                        # Extract just the text part from the result
                        result_text = result.split(": ", 1)[1] if ": " in result else result
                        st.write(result_text)

                 # Use Ollama LLM to provide a comprehensive answer
                st.markdown("## ü§ñ AI Analysis")
                 
                try:
                    from ollama import Client
                    client = Client()
                     
                     # Prepare context for LLM
                    context = "\n\n".join(relevant_chunks)

                    prompt = f"""
                    Based on the following patent document search results, provide a comprehensive answer to the user's query: "{q}"
                    
                    Search Results:
                    {context}
                     
                    Please provide:
                    1. A direct answer to the user's query based on the search results
                    2. Key insights from the most relevant results
                    3. How the search results relate to the user's question
                     
                    Answer: """
                    print("line below promt") 
                    with st.spinner("ü§ñ AI is analyzing the results..."):
                        response = client.generate(model='llama3.1', prompt=prompt)
                        ai_response = response['response']
                     
                     # Display AI response in a nice format
                    st.markdown(f"""
                    <div style="background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%); 
                                border-left: 4px solid #28a745; border-radius: 10px; 
                                padding: 1rem; margin: 1rem 0; 
                                box-shadow: 0 4px 6px rgba(0,0,0,0.1);">
                        <h4 style="margin-top: 0; color: #28a745;">üß† AI Analysis</h4>
                        <p style="margin-bottom: 0; line-height: 1.6;">{ai_response}</p>
                    </div>
                    """, unsafe_allow_html=True)
                     
                except Exception as e:
                    st.error(f"‚ùå Error with AI analysis: {e}")
                    st.info("üí° AI analysis unavailable, but you can still view the search results above.")
                                         
            except Exception as e:
                st.error(f"‚ùå Search error: {e}")

def validate_input(text: str):
    """Combine rule-based and AI-based guardrails."""
    issues = check_basic_rules(text)
    check_toxicity_result = check_toxicity(text, threshold=0.5)

    if check_toxicity_result:
        print("Detected Toxicity Categories:")
    for category, score in check_toxicity_result.items():
        print(f"  - {category}: {score:.4f}")
        return False
    else:
        print("No significant toxicity detected.")
        

    if issues:
        print("Input blocked due to:")
        for i in issues:
            print(f" - {i}")
        return False
    else:
        print("Input passed moderation.")
        return True


# Check if running with Streamlit by looking for streamlit in the call stack
import sys
import inspect

def is_running_with_streamlit():
    """Check if the script is being run by Streamlit."""
    for frame_info in inspect.stack():
        if 'streamlit' in frame_info.filename:
            return True
    return False

# If running with Streamlit, run the app directly
if is_running_with_streamlit():
    print("üöÄ Running as Streamlit app...")
    

    # If running as regular Python script
if __name__ == "__main__":
    print("üîÑ Setting up dataset and embeddings...")
    
    run_search_app()
    
    print("‚úÖ Setup complete!")
    print("üöÄ To run the search app, use: uv run streamlit run src/Semantic_Search.py")